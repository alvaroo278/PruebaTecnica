{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "dHtxB65P-1lg",
    "yEXjKRmG_JzZ",
    "FTLMm6AlXiNG",
    "orHHrNyBIJ0e",
    "bSs_E_ZqIYU8",
    "RKvheA7eo4HE"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prueba de código: Ingeniero de Datos - Orenes Core\n",
    "Esta prueba está pensada para realizar la entrevista del puesto de Ingeniero de Datos. El objetivo es que este cuaderno sirva de hilo conductor en la entrevista técnica, la prueba está pensada para que puedas mostrar lo que sabes, no para descartar o acreditar candidatos. Es una herramienta a tu disposición, úsala de la forma que creas que mejor muestra tu conocimiento o tus cualidades. Por ejemplo: Si no puedes resolver un paso puedes simular la solución para mostrar otra cosa, si piensas que hay varias formas de resolver un problema o si piensas que hay algún añadido que no se pide siéntete libre de incluirlo.\n",
    "\n",
    "\n",
    "La entrega consistirá en una copia de esta plantilla con los apartados completados y se revisará durante la entrevista. Esta plantilla no puede editarse, **ANTES DE EMPEZAR** puedes realizar una copia editable en Archivo/Guardar una Copia en Drive. Se valorará durante la entrevista si has llevado un control de versiones en git (usando GitHub por ejemplo)."
   ],
   "metadata": {
    "id": "e7rPH3A0-teT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entorno Spark\n",
    "  Esta pieza de código te permitirá usar una interfaz de Spark en Colab para resolver tus ejercicios."
   ],
   "metadata": {
    "id": "dHtxB65P-1lg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-vtskBr5mt4",
    "outputId": "ac236ce3-47b2-49b7-c651-867553416043"
   },
   "outputs": [],
   "source": [
    "# Install pyspark\n",
    "#!pip install pyspark\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a Spark Session\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "# Check Spark Session Information\n",
    "spark\n",
    "# Import a Spark function from library\n",
    "from pyspark.sql.functions import col, max"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 1\n",
    "Carga los datos del fichero housing_train.csv que guardado en la carpeta sample_data en un data frame de Spark.\n",
    "\n",
    "Se hace uso del argumento \"header\", ya que los datos vienen con cabeceras, además se usa \"inferSchema\" ya que permite que a cada columna, se le asigne el mejor tipo de dato."
   ],
   "metadata": {
    "id": "yEXjKRmG_JzZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df = spark.read.csv(\"sample_data/housing_train.csv\", header=True, inferSchema=True)\n",
    "df.columns\n",
    "df.show(5)"
   ],
   "metadata": {
    "id": "ArZY3tFPoknh"
   },
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|  -114.31|   34.19|              15.0|     5612.0|        1283.0|    1015.0|     472.0|       1.4936|           66900.0|\n",
      "|  -114.47|    34.4|              19.0|     7650.0|        1901.0|    1129.0|     463.0|         1.82|           80100.0|\n",
      "|  -114.56|   33.69|              17.0|      720.0|         174.0|     333.0|     117.0|       1.6509|           85700.0|\n",
      "|  -114.57|   33.64|              14.0|     1501.0|         337.0|     515.0|     226.0|       3.1917|           73400.0|\n",
      "|  -114.57|   33.57|              20.0|     1454.0|         326.0|     624.0|     262.0|        1.925|           65500.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 2\n",
    "Crea otro data frame a apartir de un muestreo aleatorio del 20% del data frame creado. Después crea otro data frames con el resto de registros. Comprueba que al unir los dos data frames creados no existen pérdidas ni duplicados de registors respecto al obtenido en el paso anterior."
   ],
   "metadata": {
    "id": "FTLMm6AlXiNG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se extrae el 20% de los datos haciendo uso de la función \"[sample](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.sample.html)\", que junto a su argumento \"fraction\" permite escoger el porcentaje de datos que se quiere extraer de forma ***aleatoria***, sin embargo para poder reproducir el ejercicio de forma exacta, se añade el argumento \"seed\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "muestra_df = df.sample(fraction=0.2, seed=1)"
   ],
   "metadata": {
    "id": "vDpVZla3okHZ"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para obtener un dataframe con el resto de datos, se hace usode la función \"[exceptAll](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.exceptAll.html)\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "resto_df = df.exceptAll(muestra_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación se unen ambos df, haciendo uso de la función \"[union](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.union.html)\", la cual permite realizar un merge entre dos dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "muestra_and_resto = muestra_df.union(resto_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, para comprobar que no se ha producido ninguna pérdida durante el proceso, se comprueba la longitud tanto del dataframe inicial, como el resultado tras unir ambos dataframe. Además puede darse el caso de que durante el proceso se duplique una fila y se pierda otra, nos encontrariamos en el caso en el que la primera comprobación indicaría que todo esta bien, ya que tienen el mismo número de fila, pero esto puede deverse a que tenemos la duplicada. Es por ello que con la función \"[distinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.distinct.html)\", evitamos que esto ocurra."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay pérdidas ni duplicados al unir los DataFrames\n"
     ]
    }
   ],
   "source": [
    "if muestra_and_resto.count() == df.count() and muestra_and_resto.distinct().count() == df.distinct().count():\n",
    "    print(\"No hay pérdidas ni duplicados al unir los DataFrames\")\n",
    "else:\n",
    "    print(\"Se han perdido o duplicado registros al unir los DataFrames\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 3\n",
    "Realiza un histograma de cada uno de los campos y compara los dos data frames creados en el paso anterior."
   ],
   "metadata": {
    "id": "orHHrNyBIJ0e"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 4\n",
    "Partiendo del data frame del primer paso. Obtén el registro que más al norte esté según el número de habitaciones que tenga. Es decir, el registro más al norte entre todos los registros con una sola habitación, el más al norte entre todos los registros de dos habitaciones, y así sucesivamente."
   ],
   "metadata": {
    "id": "bSs_E_ZqIYU8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se obtiene el número máximo de habitación en el dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número máximo de habitaciones:  6445.0\n"
     ]
    }
   ],
   "source": [
    "max_bedrooms = df.select(max(\"total_bedrooms\")).collect()[0][0]\n",
    "print(\"Número máximo de habitaciones: \", max_bedrooms)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación, por cada número de habitaciones, se coge del dataframe inicial, todas las filas que tenga el mismo número de habitaciones y de ellas, se escoge la habitación más al norte, es decir, ordenando de manera descendiente y cogiendo la primera fila."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "selected_records = []\n",
    "# Se recorren todos los número de habitaciones que hay\n",
    "for num_bedrooms in range(1, int(max_bedrooms)+1):\n",
    "    # Seleccionar los registros con el número de habitaciones actual\n",
    "    records = df.filter(col(\"total_bedrooms\") == num_bedrooms)\n",
    "    # Encontrar el registro con la latitud más alta\n",
    "    selected_record_list = records.orderBy(\"latitude\", ascending=False).limit(1).collect()\n",
    "    # Se comprueba que al menos exista un registro para dicho número de habitación\n",
    "    if selected_record_list:\n",
    "        # Si la lista no está vacía, obtener el primer elemento\n",
    "        selected_record = selected_record_list[0]\n",
    "        # Añadir el registro seleccionado a la lista\n",
    "        selected_records.append(selected_record)"
   ],
   "metadata": {
    "id": "-pYFshAQJ1dJ"
   },
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, se convierte la lista a un dataframe de spark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|   -122.5|   37.79|              52.0|        8.0|           1.0|      13.0|       1.0|      15.0001|          500001.0|\n",
      "|  -117.79|   35.21|               4.0|        2.0|           2.0|       6.0|       2.0|        2.375|          137500.0|\n",
      "|  -122.37|    37.6|              26.0|       15.0|           3.0|      11.0|       3.0|        5.048|          350000.0|\n",
      "|  -121.47|   38.51|              52.0|       20.0|           4.0|      74.0|       9.0|        3.625|           80000.0|\n",
      "|  -121.96|   37.13|              26.0|       50.0|           5.0|      17.0|       4.0|      15.0001|          400000.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_resultado= spark.createDataFrame(selected_records)\n",
    "df_resultado.columns\n",
    "df_resultado.show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 5\n",
    "Partiendo de los dos data frames del Paso 2. Calcula la media de las latitudes de los cada data frame agregado por el el número de dormitorios. Calcula las difrencias entre las medias de cada data frame para el mismo número de dormitorios. Es decir, la diferencia entre la latitud media de los registros del data frame del 20% con un solo dormitorio y la latitud media del data frame del 80% con un solo dormitorio; Realiza esta operación para todo número de habitaciones. "
   ],
   "metadata": {
    "id": "k0JFezm0J2KC"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "wMGbHIaFfT02"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 6\n",
    "Calcula cualquier medida que consideres interesante en utilizando Spark SQL. Por ejemplo, la media de dormitorios agrupado por las habitaciones que tiene cada registro."
   ],
   "metadata": {
    "id": "RKvheA7eo4HE"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "l4eLI2-2pU9j"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
