{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "dHtxB65P-1lg",
    "yEXjKRmG_JzZ",
    "FTLMm6AlXiNG",
    "orHHrNyBIJ0e",
    "bSs_E_ZqIYU8",
    "RKvheA7eo4HE"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prueba de código: Ingeniero de Datos - Orenes Core\n",
    "Esta prueba está pensada para realizar la entrevista del puesto de Ingeniero de Datos. El objetivo es que este cuaderno sirva de hilo conductor en la entrevista técnica, la prueba está pensada para que puedas mostrar lo que sabes, no para descartar o acreditar candidatos. Es una herramienta a tu disposición, úsala de la forma que creas que mejor muestra tu conocimiento o tus cualidades. Por ejemplo: Si no puedes resolver un paso puedes simular la solución para mostrar otra cosa, si piensas que hay varias formas de resolver un problema o si piensas que hay algún añadido que no se pide siéntete libre de incluirlo.\n",
    "\n",
    "\n",
    "La entrega consistirá en una copia de esta plantilla con los apartados completados y se revisará durante la entrevista. Esta plantilla no puede editarse, **ANTES DE EMPEZAR** puedes realizar una copia editable en Archivo/Guardar una Copia en Drive. Se valorará durante la entrevista si has llevado un control de versiones en git (usando GitHub por ejemplo)."
   ],
   "metadata": {
    "id": "e7rPH3A0-teT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entorno Spark\n",
    "  Esta pieza de código te permitirá usar una interfaz de Spark en Colab para resolver tus ejercicios."
   ],
   "metadata": {
    "id": "dHtxB65P-1lg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-vtskBr5mt4",
    "outputId": "ac236ce3-47b2-49b7-c651-867553416043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Using cached pyspark-3.3.2.tar.gz (281.4 MB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting py4j==0.10.9.5\r\n",
      "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\r\n",
      "Building wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824010 sha256=68617fc2cd11585ceeb8e4d1aa2a5fdc588da517c2ebc364466f224d9f8c04c9\r\n",
      "  Stored in directory: /home/anavarro/.cache/pip/wheels/05/fa/3e/2e840e7c1bb33325381b6b3b0e55a1c9c3e0485d2ca469229d\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: py4j, pyspark\r\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "23/03/14 15:22:38 WARN Utils: Your hostname, LAP-120-anavarro resolves to a loopback address: 127.0.1.1; using 192.168.18.172 instead (on interface wlo1)\n",
      "23/03/14 15:22:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/14 15:22:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Install pyspark\n",
    "!pip install pyspark\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a Spark Session\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "# Check Spark Session Information\n",
    "spark\n",
    "# Import a Spark function from library\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 1\n",
    "Carga los datos del fichero housing_train.csv que guardado en la carpeta sample_data en un data frame de Spark.\n",
    "\n",
    "Se hace uso del argumento \"header\", ya que los datos vienen con cabeceras, además se usa \"inferSchema\" ya que permite que a cada columna, se le asigne el mejor tipo de dato."
   ],
   "metadata": {
    "id": "yEXjKRmG_JzZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df = spark.read.csv(\"sample_data/housing_train.csv\", header=True, inferSchema=True)\n",
    "df.columns\n",
    "df.show(5)"
   ],
   "metadata": {
    "id": "ArZY3tFPoknh"
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|  -114.31|   34.19|              15.0|     5612.0|        1283.0|    1015.0|     472.0|       1.4936|           66900.0|\n",
      "|  -114.47|    34.4|              19.0|     7650.0|        1901.0|    1129.0|     463.0|         1.82|           80100.0|\n",
      "|  -114.56|   33.69|              17.0|      720.0|         174.0|     333.0|     117.0|       1.6509|           85700.0|\n",
      "|  -114.57|   33.64|              14.0|     1501.0|         337.0|     515.0|     226.0|       3.1917|           73400.0|\n",
      "|  -114.57|   33.57|              20.0|     1454.0|         326.0|     624.0|     262.0|        1.925|           65500.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 2\n",
    "Crea otro data frame a apartir de un muestreo aleatorio del 20% del data frame creado. Después crea otro data frames con el resto de registros. Comprueba que al unir los dos data frames creados no existen pérdidas ni duplicados de registors respecto al obtenido en el paso anterior."
   ],
   "metadata": {
    "id": "FTLMm6AlXiNG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se extrae el 20% de los datos haciendo uso de la función \"[sample](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.sample.html)\", que junto a su argumento \"fraction\" permite escoger el porcentaje de datos que se quiere extraer de forma ***aleatoria***, sin embargo para poder reproducir el ejercicio de forma exacta, se añade el argumento \"seed\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "muestra_df = df.sample(fraction=0.2, seed=1)"
   ],
   "metadata": {
    "id": "vDpVZla3okHZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para obtener un dataframe con el resto de datos, se hace usode la función \"[exceptAll](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.exceptAll.html)\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resto_df = df.exceptAll(muestra_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación se unen ambos df, haciendo uso de la función \"[union](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.union.html)\", la cual permite realizar un merge entre dos dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "muestra_and_resto = muestra_df.union(resto_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, para comprobar que no se ha producido ninguna pérdida durante el proceso, se comprueba la longitud tanto del dataframe inicial, como el resultado tras unir ambos dataframe. Además puede darse el caso de que durante el proceso se duplique una fila y se pierda otra, nos encontrariamos en el caso en el que la primera comprobación indicaría que todo esta bien, ya que tienen el mismo número de fila, pero esto puede deverse a que tenemos la duplicada. Es por ello que con la función \"[distinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.distinct.html)\", evitamos que esto ocurra."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if muestra_and_resto.count() == df.count() and muestra_and_resto.distinct().count() == df.distinct().count():\n",
    "    print(\"No hay pérdidas ni duplicados al unir los DataFrames\")\n",
    "else:\n",
    "    print(\"Se han perdido o duplicado registros al unir los DataFrames\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 3\n",
    "Realiza un histograma de cada uno de los campos y compara los dos data frames creados en el paso anterior."
   ],
   "metadata": {
    "id": "orHHrNyBIJ0e"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 4\n",
    "Partiendo del data frame del primer paso. Obtén el registro que más al norte esté según el número de habitaciones que tenga. Es decir, el registro más al norte entre todos los registros con una sola habitación, el más al norte entre todos los registros de dos habitaciones, y así sucesivamente. "
   ],
   "metadata": {
    "id": "bSs_E_ZqIYU8"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "-pYFshAQJ1dJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 5\n",
    "Partiendo de los dos data frames del Paso 2. Calcula la media de las latitudes de los cada data frame agregado por el el número de dormitorios. Calcula las difrencias entre las medias de cada data frame para el mismo número de dormitorios. Es decir, la diferencia entre la latitud media de los registros del data frame del 20% con un solo dormitorio y la latitud media del data frame del 80% con un solo dormitorio; Realiza esta operación para todo número de habitaciones. "
   ],
   "metadata": {
    "id": "k0JFezm0J2KC"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "wMGbHIaFfT02"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 6\n",
    "Calcula cualquier medida que consideres interesante en utilizando Spark SQL. Por ejemplo, la media de dormitorios agrupado por las habitaciones que tiene cada registro."
   ],
   "metadata": {
    "id": "RKvheA7eo4HE"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "l4eLI2-2pU9j"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
