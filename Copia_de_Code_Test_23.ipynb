{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "dHtxB65P-1lg",
    "yEXjKRmG_JzZ",
    "FTLMm6AlXiNG",
    "orHHrNyBIJ0e",
    "bSs_E_ZqIYU8",
    "RKvheA7eo4HE"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prueba de código: Ingeniero de Datos - Orenes Core\n",
    "Esta prueba está pensada para realizar la entrevista del puesto de Ingeniero de Datos. El objetivo es que este cuaderno sirva de hilo conductor en la entrevista técnica, la prueba está pensada para que puedas mostrar lo que sabes, no para descartar o acreditar candidatos. Es una herramienta a tu disposición, úsala de la forma que creas que mejor muestra tu conocimiento o tus cualidades. Por ejemplo: Si no puedes resolver un paso puedes simular la solución para mostrar otra cosa, si piensas que hay varias formas de resolver un problema o si piensas que hay algún añadido que no se pide siéntete libre de incluirlo.\n",
    "\n",
    "\n",
    "La entrega consistirá en una copia de esta plantilla con los apartados completados y se revisará durante la entrevista. Esta plantilla no puede editarse, **ANTES DE EMPEZAR** puedes realizar una copia editable en Archivo/Guardar una Copia en Drive. Se valorará durante la entrevista si has llevado un control de versiones en git (usando GitHub por ejemplo)."
   ],
   "metadata": {
    "id": "e7rPH3A0-teT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entorno Spark\n",
    "  Esta pieza de código te permitirá usar una interfaz de Spark en Colab para resolver tus ejercicios."
   ],
   "metadata": {
    "id": "dHtxB65P-1lg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-vtskBr5mt4",
    "outputId": "ac236ce3-47b2-49b7-c651-867553416043"
   },
   "outputs": [],
   "source": [
    "# Install pyspark\n",
    "#!pip install pyspark\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark Session\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "# Check Spark Session Information\n",
    "spark\n",
    "# Import a Spark function from library\n",
    "from pyspark.sql.functions import col, max, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 1\n",
    "Carga los datos del fichero housing_train.csv que guardado en la carpeta sample_data en un data frame de Spark.\n",
    "\n",
    "Se hace uso del argumento \"header\", ya que los datos vienen con cabeceras, además se usa \"inferSchema\" ya que permite que a cada columna, se le asigne el mejor tipo de dato."
   ],
   "metadata": {
    "id": "yEXjKRmG_JzZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_inicial = spark.read.csv(\"sample_data/housing_train.csv\", header=True, inferSchema=True)\n",
    "dataset_inicial.columns\n",
    "dataset_inicial.show(5)"
   ],
   "metadata": {
    "id": "ArZY3tFPoknh"
   },
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|  -114.31|   34.19|              15.0|     5612.0|        1283.0|    1015.0|     472.0|       1.4936|           66900.0|\n",
      "|  -114.47|    34.4|              19.0|     7650.0|        1901.0|    1129.0|     463.0|         1.82|           80100.0|\n",
      "|  -114.56|   33.69|              17.0|      720.0|         174.0|     333.0|     117.0|       1.6509|           85700.0|\n",
      "|  -114.57|   33.64|              14.0|     1501.0|         337.0|     515.0|     226.0|       3.1917|           73400.0|\n",
      "|  -114.57|   33.57|              20.0|     1454.0|         326.0|     624.0|     262.0|        1.925|           65500.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 2\n",
    "Crea otro data frame a apartir de un muestreo aleatorio del 20% del data frame creado. Después crea otro data frames con el resto de registros. Comprueba que al unir los dos data frames creados no existen pérdidas ni duplicados de registors respecto al obtenido en el paso anterior."
   ],
   "metadata": {
    "id": "FTLMm6AlXiNG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se extrae el 20% de los datos haciendo uso de la función \"[sample](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.sample.html)\", que junto a su argumento \"fraction\" permite escoger el porcentaje de datos que se quiere extraer de forma ***aleatoria***, sin embargo para poder reproducir el ejercicio de forma exacta, se añade el argumento \"seed\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "muestra_df = dataset_inicial.sample(fraction=0.2, seed=1)"
   ],
   "metadata": {
    "id": "vDpVZla3okHZ"
   },
   "execution_count": 97,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para obtener un dataframe con el resto de datos, se hace usode la función \"[exceptAll](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.exceptAll.html)\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "resto_df = dataset_inicial.exceptAll(muestra_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación se unen ambos df, haciendo uso de la función \"[union](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.union.html)\", la cual permite realizar un merge entre dos dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "muestra_and_resto = muestra_df.union(resto_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, para comprobar que no se ha producido ninguna pérdida durante el proceso, se comprueba la longitud tanto del dataframe inicial, como el resultado tras unir ambos dataframe. Además puede darse el caso de que durante el proceso se duplique una fila y se pierda otra, nos encontrariamos en el caso en el que la primera comprobación indicaría que todo esta bien, ya que tienen el mismo número de fila, pero esto puede deverse a que tenemos la duplicada. Es por ello que con la función \"[distinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.distinct.html)\", evitamos que esto ocurra."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay pérdidas ni duplicados al unir los DataFrames\n"
     ]
    }
   ],
   "source": [
    "if muestra_and_resto.count() == dataset_inicial.count() and muestra_and_resto.distinct().count() == dataset_inicial.distinct().count():\n",
    "    print(\"No hay pérdidas ni duplicados al unir los DataFrames\")\n",
    "else:\n",
    "    print(\"Se han perdido o duplicado registros al unir los DataFrames\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 3\n",
    "Realiza un histograma de cada uno de los campos y compara los dos data frames creados en el paso anterior."
   ],
   "metadata": {
    "id": "orHHrNyBIJ0e"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 4\n",
    "Partiendo del data frame del primer paso. Obtén el registro que más al norte esté según el número de habitaciones que tenga. Es decir, el registro más al norte entre todos los registros con una sola habitación, el más al norte entre todos los registros de dos habitaciones, y así sucesivamente."
   ],
   "metadata": {
    "id": "bSs_E_ZqIYU8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se obtiene el número máximo de habitación en el dataframe. A continuación, por cada número de habitaciones, se coge del dataframe inicial, todas las filas que tenga el mismo número de habitaciones y de ellas, se escoge la habitación más al norte, es decir, ordenando de manera descendiente y cogiendo la primera fila."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def select_records_by_num_bedrooms_v1(df):\n",
    "    max_bedrooms = df.select(max(\"total_bedrooms\")).collect()[0][0]\n",
    "    selected_records = []\n",
    "    # Se recorren todos los número de habitaciones que hay\n",
    "    for num_bedrooms in range(1, int(max_bedrooms) + 1):\n",
    "        # Se seleccionan los registros con el número de habitaciones actual\n",
    "        records = df.filter(col(\"total_bedrooms\") == num_bedrooms)\n",
    "        selected_record = records.orderBy(\"latitude\", ascending=False).limit(1).collect()\n",
    "        # Se comprueba que exista un registro para dicho número de abitación\n",
    "        if selected_record:\n",
    "            selected_record = selected_record[0]\n",
    "            # Añadir el registro seleccionado a la lista\n",
    "            selected_records.append(selected_record)\n",
    "    return spark.createDataFrame(selected_records)"
   ],
   "metadata": {
    "id": "-pYFshAQJ1dJ"
   },
   "execution_count": 101,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La opción anterior, tarda demasiado tiempo, ya que se hace un uso de un for, por lo que hay que ir iterando, sin embargo es más legible entender el código. Otra opción sería:\n",
    "    - Agrupo por \"total_bedrroms\" (groupBy)\n",
    "    - De cada agrupación, se coge la fila de mayor valor de latitude (max)\n",
    "\n",
    "De esta manera haciendo uso de \"groupBy\" y de \"agg\", todo puede ser más rápido."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "def select_records_by_num_bedrooms_v2(df):\n",
    "    # Se agrupa por número de habitaciones y se obtiene el registro más al norte para cada grupo\n",
    "    max_latitude_by_rooms = (dataset_inicial\n",
    "                             .groupBy(\"total_bedrooms\")\n",
    "                             .agg(max(\"latitude\").alias(\"max_latitude\")))\n",
    "    # Se une el DataFrame original con los registros más al norte para cada grupo\n",
    "    joined_df = dataset_inicial.join(max_latitude_by_rooms, \"total_bedrooms\")\n",
    "    # Se selecciona solo las columnas deseadas y ordenar por número de habitaciones\n",
    "    result_df = (joined_df\n",
    "                 .select(\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\",\n",
    "                         \"households\", \"median_income\", \"median_house_value\")\n",
    "                 .orderBy(\"total_bedrooms\", ascendent=False))\n",
    "    return result_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Comprobación del tiempo que tardan**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iterando con un for."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de la función utilizando for:  350.05750012397766\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|   -122.5|   37.79|              52.0|        8.0|           1.0|      13.0|       1.0|      15.0001|          500001.0|\n",
      "|  -117.79|   35.21|               4.0|        2.0|           2.0|       6.0|       2.0|        2.375|          137500.0|\n",
      "|  -122.37|    37.6|              26.0|       15.0|           3.0|      11.0|       3.0|        5.048|          350000.0|\n",
      "|  -121.47|   38.51|              52.0|       20.0|           4.0|      74.0|       9.0|        3.625|           80000.0|\n",
      "|  -121.96|   37.13|              26.0|       50.0|           5.0|      17.0|       4.0|      15.0001|          400000.0|\n",
      "|  -122.39|    38.0|              33.0|       44.0|           6.0|      23.0|      11.0|        4.125|          212500.0|\n",
      "|  -119.54|   36.51|              36.0|       49.0|           7.0|      28.0|       2.0|        4.625|          162500.0|\n",
      "|  -120.13|   35.87|              26.0|       48.0|           8.0|      13.0|       8.0|        2.375|           71300.0|\n",
      "|   -122.4|   37.75|              26.0|       54.0|           9.0|      23.0|       9.0|       6.1359|          225000.0|\n",
      "|  -122.05|   37.97|              16.0|       60.0|          10.0|      65.0|      19.0|       6.1359|          250000.0|\n",
      "|  -122.42|   37.63|              46.0|       66.0|          11.0|      30.0|      12.0|        2.375|          275000.0|\n",
      "|  -120.96|   37.64|              36.0|       60.0|          12.0|      51.0|      14.0|        3.625|           67500.0|\n",
      "|  -117.11|   33.12|              46.0|       52.0|          13.0|      59.0|      13.0|        3.875|          200000.0|\n",
      "|   -118.3|   34.17|              30.0|       48.0|          14.0|      74.0|      16.0|       5.0056|          162500.0|\n",
      "|  -122.41|   37.98|              36.0|       60.0|          15.0|      42.0|      25.0|       1.4583|           67500.0|\n",
      "|  -121.28|   38.63|              36.0|      120.0|          16.0|      30.0|      14.0|      10.2264|          350000.0|\n",
      "|  -121.52|   39.12|              37.0|      102.0|          17.0|      29.0|      14.0|        4.125|           72000.0|\n",
      "|  -122.91|   39.18|              43.0|       89.0|          18.0|      86.0|      27.0|       2.0208|           72500.0|\n",
      "|  -120.19|   38.07|              43.0|      102.0|          19.0|      44.0|      13.0|       0.4999|          162500.0|\n",
      "|  -120.95|   37.65|              37.0|      136.0|          20.0|      72.0|      22.0|       2.2279|          225000.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se mide el tiempo de ejecución\n",
    "star_time_v1 = time.time()\n",
    "dataset_resultado = select_records_by_num_bedrooms_v1(dataset_inicial)\n",
    "elapsed_time_v1 = time.time() - star_time_v1\n",
    "print(\"Tiempo de ejecución de la función utilizando for: \", elapsed_time_v1)\n",
    "\n",
    "# Se muestra el resultado\n",
    "dataset_resultado.columns\n",
    "dataset_resultado.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utilizando \"groupBy\" y \"agg\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de la función optimizada:  0.022614717483520508\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|   -122.5|   37.79|              52.0|        8.0|           1.0|      13.0|       1.0|      15.0001|          500001.0|\n",
      "|  -117.79|   35.21|               4.0|        2.0|           2.0|       6.0|       2.0|        2.375|          137500.0|\n",
      "|  -117.27|   34.17|              16.0|       30.0|           3.0|      49.0|       8.0|        4.625|          250000.0|\n",
      "|  -122.37|    37.6|              26.0|       15.0|           3.0|      11.0|       3.0|        5.048|          350000.0|\n",
      "|  -117.76|   35.22|               4.0|       18.0|           3.0|       8.0|       6.0|        1.625|          275000.0|\n",
      "|  -119.23|   34.25|              28.0|       26.0|           3.0|      29.0|       9.0|          8.0|          275000.0|\n",
      "|  -122.29|   37.81|              46.0|       12.0|           4.0|      18.0|       7.0|       0.4999|           67500.0|\n",
      "|  -120.85|   37.75|              26.0|       28.0|           4.0|       9.0|       5.0|        1.625|           85000.0|\n",
      "|  -122.14|    37.5|              46.0|       30.0|           4.0|      13.0|       5.0|      15.0001|          500001.0|\n",
      "|   -122.0|    37.0|              16.0|       32.0|           4.0|      36.0|       5.0|        2.625|          137500.0|\n",
      "|  -121.47|   38.51|              52.0|       20.0|           4.0|      74.0|       9.0|        3.625|           80000.0|\n",
      "|  -122.06|   37.39|              26.0|       18.0|           4.0|       8.0|       4.0|         3.75|          375000.0|\n",
      "|  -117.86|   33.67|              16.0|       20.0|           5.0|      15.0|       5.0|        3.875|          450000.0|\n",
      "|  -118.04|    33.9|              36.0|       15.0|           5.0|      15.0|       6.0|       0.4999|          162500.0|\n",
      "|  -117.19|   32.75|              52.0|       25.0|           5.0|      13.0|       5.0|        0.536|          162500.0|\n",
      "|  -121.96|   37.13|              26.0|       50.0|           5.0|      17.0|       4.0|      15.0001|          400000.0|\n",
      "|  -117.75|   34.06|              52.0|       24.0|           6.0|      46.0|       7.0|        1.625|           67500.0|\n",
      "|  -118.22|   34.06|              52.0|       48.0|           6.0|      41.0|      10.0|      10.2264|          112500.0|\n",
      "|  -118.44|   34.04|              16.0|       18.0|           6.0|       3.0|       4.0|        0.536|          350000.0|\n",
      "|   -121.3|   37.94|              52.0|       24.0|           6.0|      23.0|       5.0|        2.375|           67500.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se mide el tiempo de ejecución\n",
    "star_time_v2 = time.time()\n",
    "dataset_resultado_v2 = select_records_by_num_bedrooms_v2(dataset_inicial)\n",
    "elapsed_time_v2 = time.time() - star_time_v2\n",
    "print(\"Tiempo de ejecución de la función optimizada: \", elapsed_time_v2)\n",
    "\n",
    "# Se muestra el resultado\n",
    "dataset_resultado_v2.columns\n",
    "dataset_resultado_v2.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 5\n",
    "Partiendo de los dos data frames del Paso 2. Calcula la media de las latitudes de los cada data frame agregado por el el número de dormitorios. Calcula las difrencias entre las medias de cada data frame para el mismo número de dormitorios. Es decir, la diferencia entre la latitud media de los registros del data frame del 20% con un solo dormitorio y la latitud media del data frame del 80% con un solo dormitorio; Realiza esta operación para todo número de habitaciones."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se comienza, agrupando por la columna \"total_bedrooms\", seguidamente con la función \"[agg](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.agg.html)\", se calcula la media de cada grupo y finalmente se se nombra la columna."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "media_muestra = muestra_df.groupBy(\"total_bedrooms\").agg(mean(\"latitude\").alias(\"mean_latitude_muestra\")).orderBy(\"total_bedrooms\", ascendent=False)\n",
    "print(\"Resultado dataset 20%\")\n",
    "media_muestra.show()\n",
    "\n",
    "media_resto = resto_df.groupBy(\"total_bedrooms\").agg(mean(\"latitude\").alias(\"mean_latitude_resto\")).orderBy(\"total_bedrooms\", ascendent=False)\n",
    "print(\"Resultado dataset resto\")\n",
    "media_resto.show()"
   ],
   "metadata": {
    "id": "wMGbHIaFfT02"
   },
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado dataset 20%\n",
      "+--------------+---------------------+\n",
      "|total_bedrooms|mean_latitude_muestra|\n",
      "+--------------+---------------------+\n",
      "|           3.0|                 37.6|\n",
      "|           4.0|               37.625|\n",
      "|           5.0|                32.75|\n",
      "|           9.0|                34.02|\n",
      "|          10.0|                36.06|\n",
      "|          11.0|    34.48333333333333|\n",
      "|          12.0|               35.675|\n",
      "|          13.0|                33.12|\n",
      "|          14.0|                 33.8|\n",
      "|          15.0|                37.98|\n",
      "|          16.0|                35.24|\n",
      "|          17.0|                35.32|\n",
      "|          18.0|   36.449999999999996|\n",
      "|          19.0|                33.83|\n",
      "|          20.0|                35.78|\n",
      "|          21.0|                38.59|\n",
      "|          22.0|               34.045|\n",
      "|          23.0|                33.96|\n",
      "|          25.0|                33.28|\n",
      "|          28.0|               35.715|\n",
      "+--------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Resultado dataset resto\n",
      "+--------------+-------------------+\n",
      "|total_bedrooms|mean_latitude_resto|\n",
      "+--------------+-------------------+\n",
      "|           1.0|              37.79|\n",
      "|           2.0|              35.21|\n",
      "|           3.0|  34.54666666666667|\n",
      "|           4.0|            37.6775|\n",
      "|           5.0|               34.9|\n",
      "|           6.0|              35.62|\n",
      "|           7.0|            34.5925|\n",
      "|           8.0|             34.945|\n",
      "|           9.0|             34.615|\n",
      "|          10.0|             35.895|\n",
      "|          11.0| 35.527499999999996|\n",
      "|          12.0| 35.089999999999996|\n",
      "|          14.0|             33.425|\n",
      "|          16.0|               35.8|\n",
      "|          17.0|             37.508|\n",
      "|          18.0|               35.5|\n",
      "|          19.0|              38.07|\n",
      "|          20.0|  35.63333333333333|\n",
      "|          21.0|  37.72666666666667|\n",
      "|          22.0|              34.07|\n",
      "+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Una vez que ya tenemos cada tabla con su respectiva media para cada número de habitación, se realizan los siguientes pasos:\n",
    "    - Se unen ambos dataframe, mediante la columna total_bedrooms, para así tener el número de habitación con sus correspondientes medias de cada dataframe.\n",
    "    - Se crea la columna \"diferencia\", compuesta por la resta de cada una de las medias.\n",
    "    - Finalmente se ordena de en función de la columna habitaciones."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+\n",
      "|total_bedrooms|         diferencia|\n",
      "+--------------+-------------------+\n",
      "|        1856.0| -4.979999999999997|\n",
      "|        1655.0| -4.899999999999999|\n",
      "|        1324.0| -4.899999999999999|\n",
      "|        1039.0| -4.630000000000003|\n",
      "|        1542.0| -4.580000000000005|\n",
      "|        1158.0| -4.479999999999997|\n",
      "|          19.0| -4.240000000000002|\n",
      "|          31.0| -4.229999999999997|\n",
      "|        1893.0| -4.130000000000003|\n",
      "|         858.0| -4.048333333333339|\n",
      "|        1110.0| -4.035000000000004|\n",
      "|        1213.0|-3.9299999999999997|\n",
      "|         992.0|-3.9279999999999973|\n",
      "|        1914.0|-3.8900000000000006|\n",
      "|         714.0| -3.865000000000002|\n",
      "|         117.0|-3.7674999999999983|\n",
      "|        1575.0| -3.719999999999999|\n",
      "|        1050.0|-3.6099999999999994|\n",
      "|         870.0| -3.604999999999997|\n",
      "|         907.0| -3.520000000000003|\n",
      "+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diferencias = media_muestra.join(media_resto, \"total_bedrooms\") \\\n",
    "    .withColumn(\"diferencia\", col(\"mean_latitude_muestra\") - col(\"mean_latitude_resto\")) \\\n",
    "    .orderBy(\"diferencia\", ascendent=False) \\\n",
    "    .select(\"total_bedrooms\", \"diferencia\")\n",
    "diferencias.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paso 6\n",
    "Calcula cualquier medida que consideres interesante en utilizando Spark SQL. Por ejemplo, la media de dormitorios agrupado por las habitaciones que tiene cada registro."
   ],
   "metadata": {
    "id": "RKvheA7eo4HE"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "l4eLI2-2pU9j"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
